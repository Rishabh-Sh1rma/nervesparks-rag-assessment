import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_openai import OpenAI

class RAGPipeline:
    """
    A class to encapsulate the entire Retrieval-Augmented Generation pipeline.
    
    This class handles:
    1. Loading documents from a directory.
    2. Splitting documents into manageable chunks.
    3. Creating a vector store using embeddings.
    4. Answering questions based on the document store.
    """
    def __init__(self, repo_path: str):
        """
        Initializes the RAG pipeline.

        Args:
            repo_path (str): The local path to the cloned code repository.
        """
        self.repo_path = repo_path
        self.vector_store = None
        self.qa_chain = None
        # A list of common file extensions for code and text to process.
        # This helps in focusing only on relevant files.
        self.supported_extensions = [
            ".py", ".js", ".ts", ".java", ".c", ".cpp", ".h", ".cs", ".go",
            ".html", ".css", ".scss", ".md", ".json", ".xml", ".yaml", ".yml",
            ".txt", "Dockerfile", ".sh"
        ]

    def _load_and_split_documents(self):
        """
        Loads all supported files from the repository path and splits them into chunks.
        
        This method walks through the repository directory, identifies supported file types,
        and uses a RecursiveCharacterTextSplitter. This splitter is chosen because it's
        effective for code, as it tries to split along logical boundaries like newlines,
        functions, and classes.
        """
        documents = []
        for dirpath, _, filenames in os.walk(self.repo_path):
            # We explicitly ignore the .git directory as it contains metadata, not source code.
            if ".git" in dirpath:
                continue
            for filename in filenames:
                if any(filename.endswith(ext) for ext in self.supported_extensions):
                    file_path = os.path.join(dirpath, filename)
                    try:
                        loader = TextLoader(file_path, encoding='utf-8')
                        documents.extend(loader.load())
                    except Exception as e:
                        print(f"Failed to load {file_path}: {e}")
        
        # This text splitter is configured for code, with some overlap to maintain context
        # between chunks, which is crucial for understanding code flow.
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
        return text_splitter.split_documents(documents)

    def initialize(self):
        """
        Initializes the full pipeline: loads documents, creates vector store, and sets up QA chain.
        This method orchestrates the setup process and should be called before asking questions.
        """
        print("Loading and splitting documents...")
        split_docs = self._load_and_split_documents()
        
        # If no documents were found, we cannot proceed.
        if not split_docs:
            print("No supported documents found to process.")
            return

        print("Creating vector store from documents...")
        # OpenAI embeddings are used for their high quality in understanding both
        # natural language and code semantics.
        embeddings = OpenAIEmbeddings()
        # Chroma is used as the vector store. 'from_documents' creates and stores
        # the embeddings in memory for fast retrieval.
        self.vector_store = Chroma.from_documents(documents=split_docs, embedding=embeddings)
        
        print("Setting up the Question-Answering chain...")
        # The RetrievalQA chain combines the retriever (our vector store) and an LLM
        # to answer questions. 'stuff' method passes all retrieved chunks into the prompt.
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=OpenAI(temperature=0.2), # Lower temperature for more factual, less creative answers
            chain_type="stuff",
            retriever=self.vector_store.as_retriever()
        )
        print("Pipeline initialized successfully.")

    def ask_question(self, question: str) -> str:
        """
        Asks a question to the initialized QA chain.

        Args:
            question (str): The question to ask about the codebase.

        Returns:
            str: The answer generated by the pipeline.
        """
        if not self.qa_chain:
            return "The pipeline has not been initialized. Please run the initialize method first."
        
        print(f"Asking question: {question}")
        result = self.qa_chain.invoke(question)
        return result['result']